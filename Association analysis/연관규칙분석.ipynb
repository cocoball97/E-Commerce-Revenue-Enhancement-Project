{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fe7cdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 협업 필터링에는 2가지가 있음\n",
    "\n",
    "# 1. 고객별 개개인 추천시스템 (사용자 추천 서비스)\n",
    "# 2. 일반적으로 관련된 상품 추천 (아이템 추천 서비스)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89314bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 고객을 전부 넣어야 하는 이유\n",
    "\n",
    "# 고객이 상품을 하나만 구매할 때 해당 상품이 더 많이 등장한다는 사실로부터 우리는 다음과 같은 정보를 얻을 수 있습니다.\n",
    "\n",
    "# 상품의 인기도: 해당 상품이 다른 상품들보다 인기가 높다면, 고객이 단 하나의 상품을 구매할 때에도 해당 상품이 더 많이 등장할 가능성이 높아집니다. \n",
    "# 이는 해당 상품이 고객들에게 매력적인 제품이라는 증거가 됩니다.\n",
    "\n",
    "# 고객의 취향: 고객이 단 하나의 상품을 선택할 때 해당 상품이 다른 상품들보다 더 많이 등장한다면,\n",
    "# 이는 해당 상품이 고객의 취향에 맞는 제품이라는 증거가 됩니다. 이를 통해 상품을 더욱 정확하게 타겟팅하여 마케팅 전략을 수립할 수 있습니다.\n",
    "\n",
    "########\n",
    "# 크로스셀링 기회: 단 하나의 상품만을 구매하는 고객에게 해당 상품과 관련된 다른 상품들을 제안함으로써 추가적인 구매 기회를 창출할 수 있습니다.  \n",
    "# 예를 들어, 만약 해당 상품이 어떤 다른 제품과 함께 사용하는 것이 일반적이라면, 해당 제품과 함께 구매할 만한 크로스셀링 기회가 있을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77ca4a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한 개만 구매하는 고객을 빼는 경우\n",
    "\n",
    "# 한 개만 구매하는 고객을 빼고 연관분석을 수행하는 경우는 \"단일 구매자 분석\"이라고도 불리며, 이 경우 다음과 같은 정보를 얻을 수 있습니다.\n",
    "\n",
    "# 상품 번들링 기회: 연관분석을 통해 다른 고객들이 함께 구매하는 상품들을 파악함으로써, 해당 상품과 함께 구매되는 다른 상품들을 제안할 수 있습니다. \n",
    "# 이를 통해 상품 번들링 기회를 찾아내어 매출을 높일 수 있습니다.\n",
    "\n",
    "# 상품 포지셔닝: 연관분석 결과를 통해 어떤 상품들이 함께 구매되는지 파악함으로써, 해당 상품의 위치와 경쟁 업체와의 차별화 전략을 수립할 수 있습니다.\n",
    "\n",
    "# 상품 품질 개선: 연관분석을 통해 함께 구매되는 상품들 중에 불만족스러운 상품이 있다면, 해당 상품의 품질을 개선하여 고객 만족도를 높일 수 있습니다.\n",
    "\n",
    "# 고객 세분화: 단일 구매자 분석을 통해 한 개만 구매하는 고객들의 구매 패턴을 파악할 수 있습니다. \n",
    "# 이를 기반으로 고객을 세분화하여 타겟팅을 더욱 정확하게 할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68982f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1개짜리도 포함시킨 이유\n",
    "\n",
    "# 만약, 여러 번 구매하지만 상품 하나만 구매하는 경우가 다른 고객들과 함께 구매하는 상품들의 교집합이 크다면,\n",
    "# 들을 제외하면 추천시스템이 불완전해질 수 있습니다. 따라서, 이러한 경우에는 여러 번 구매하지만 상품 하나만 구매하는 경우도 모델에 포함시켜야 합니다.\n",
    "\n",
    "# 다른 고객들과 함께 구매하는 상품들의 교집합이 크다: 여러 번 구매하지만 상품 하나만 구매하는 경우가 다른 고객들과 함께 구매하는 상품들이 많이 겹친다\n",
    "\n",
    "\n",
    "# 그러나, 여러 번 구매하지만 상품 하나만 구매하는 경우가 다른 고객들과 함께 구매하는 상품들의 교집합이 작거나 없는 경우에는 \n",
    "# 이들을 모델에서 제외시켜도 괜찮습니다. 이러한 경우에는 다른 고객들과 구매 패턴이 다르기 때문에,\n",
    "# 이들의 데이터를 모델에 포함시키면 노이즈로 작용하여 추천 정확도를 떨어뜨릴 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae5fb66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib\n",
    "%matplotlib inline \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "matplotlib.rc(\"font\",family = \"NaNumGothic\")\n",
    "matplotlib.rc(\"axes\",unicode_minus = False) # 음수표시 \n",
    "\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1411d0c1",
   "metadata": {},
   "source": [
    "# 연령별"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7890d54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 267 combinations | Sampling itemset size 32\n"
     ]
    }
   ],
   "source": [
    "df= pd.read_csv(\"../data/구매상품데이터프레임 20대.csv\",index_col=0)\n",
    "\n",
    "records = []\n",
    "for i in range(len(df)):\n",
    "    records.append([str(df.values[i,j]) \\\n",
    "                    for j in range(len(df.columns)) if not pd.isna(df.values[i,j])])\n",
    "\n",
    "\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(records).transform(records, sparse=True)\n",
    "te_df = pd.DataFrame.sparse.from_spmatrix(te_ary, columns=te.columns_)\n",
    "\n",
    "\n",
    "frequent_itemset = apriori(te_df,\n",
    "                           min_support=0.005, \n",
    "                           max_len=3, \n",
    "                           use_colnames=True, \n",
    "                           verbose=1 \n",
    "                          )\n",
    "frequent_itemset['length'] = frequent_itemset['itemsets'].map(lambda x: len(x))\n",
    "frequent_itemset.sort_values('support',ascending=False,inplace=True)\n",
    "\n",
    "\n",
    "association_rules_df = association_rules(frequent_itemset, \n",
    "                                         metric='confidence', \n",
    "                                         min_threshold=0.005,\n",
    "                                        )\n",
    "all_confidences = []\n",
    "collective_strengths = []\n",
    "cosine_similarities = []\n",
    "for _,row in association_rules_df.iterrows():\n",
    "    all_confidence_if = list(row['antecedents'])[0]\n",
    "    all_confidence_then = list(row['consequents'])[0]\n",
    "    if row['antecedent support'] <= row['consequent support']:\n",
    "        all_confidence_if = list(row['consequents'])[0]\n",
    "        all_confidence_then = list(row['antecedents'])[0]\n",
    "    all_confidence = {all_confidence_if+' => '+all_confidence_then : \\\n",
    "                      row['support']/max(row['antecedent support'], row['consequent support'])}\n",
    "    all_confidences.append(all_confidence)\n",
    "    \n",
    "    violation = row['antecedent support'] + row['consequent support'] - 2*row['support']\n",
    "    ex_violation = 1-row['antecedent support']*row['consequent support'] - \\\n",
    "                    (1-row['antecedent support'])*(1-row['consequent support'])\n",
    "    if violation == 0 or ex_violation == 0:\n",
    "        collective_strength = 0\n",
    "    else:\n",
    "        collective_strength = (1-violation)/(1-ex_violation)*(ex_violation/violation+1)+1\n",
    "    collective_strengths.append(collective_strength)\n",
    "    \n",
    "    cosine_similarity = row['support']/np.sqrt(row['antecedent support']*row['consequent support'])\n",
    "    cosine_similarities.append(cosine_similarity)\n",
    "    \n",
    "association_rules_df['all-confidence'] = all_confidences\n",
    "association_rules_df['collective strength'] = collective_strengths\n",
    "association_rules_df['cosine similarity'] = cosine_similarities\n",
    "\n",
    "df_association = association_rules_df.sort_values(by='lift',ascending=False)\n",
    "df_association\n",
    "\n",
    "df_association.to_csv('../data/연관규칙 20대.csv', encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a857344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 273 combinations | Sampling itemset size 32\n"
     ]
    }
   ],
   "source": [
    "df= pd.read_csv(\"../data/구매상품데이터프레임 30대.csv\",index_col=0)\n",
    "\n",
    "records = []\n",
    "for i in range(len(df)):\n",
    "    records.append([str(df.values[i,j]) \\\n",
    "                    for j in range(len(df.columns)) if not pd.isna(df.values[i,j])])\n",
    "\n",
    "\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(records).transform(records, sparse=True)\n",
    "te_df = pd.DataFrame.sparse.from_spmatrix(te_ary, columns=te.columns_)\n",
    "\n",
    "frequent_itemset = apriori(te_df,\n",
    "                           min_support=0.005, \n",
    "                           max_len=3, \n",
    "                           use_colnames=True, \n",
    "                           verbose=1 \n",
    "                          )\n",
    "frequent_itemset['length'] = frequent_itemset['itemsets'].map(lambda x: len(x))\n",
    "frequent_itemset.sort_values('support',ascending=False,inplace=True)\n",
    "\n",
    "\n",
    "association_rules_df = association_rules(frequent_itemset, \n",
    "                                         metric='confidence', \n",
    "                                         min_threshold=0.005,\n",
    "                                        )\n",
    "all_confidences = []\n",
    "collective_strengths = []\n",
    "cosine_similarities = []\n",
    "for _,row in association_rules_df.iterrows():\n",
    "    all_confidence_if = list(row['antecedents'])[0]\n",
    "    all_confidence_then = list(row['consequents'])[0]\n",
    "    if row['antecedent support'] <= row['consequent support']:\n",
    "        all_confidence_if = list(row['consequents'])[0]\n",
    "        all_confidence_then = list(row['antecedents'])[0]\n",
    "    all_confidence = {all_confidence_if+' => '+all_confidence_then : \\\n",
    "                      row['support']/max(row['antecedent support'], row['consequent support'])}\n",
    "    all_confidences.append(all_confidence)\n",
    "    \n",
    "    violation = row['antecedent support'] + row['consequent support'] - 2*row['support']\n",
    "    ex_violation = 1-row['antecedent support']*row['consequent support'] - \\\n",
    "                    (1-row['antecedent support'])*(1-row['consequent support'])\n",
    "    if violation == 0 or ex_violation == 0:\n",
    "        collective_strength = 0\n",
    "    else:\n",
    "        collective_strength = (1-violation)/(1-ex_violation)*(ex_violation/violation+1)+1\n",
    "    collective_strengths.append(collective_strength)\n",
    "    \n",
    "    cosine_similarity = row['support']/np.sqrt(row['antecedent support']*row['consequent support'])\n",
    "    cosine_similarities.append(cosine_similarity)\n",
    "    \n",
    "association_rules_df['all-confidence'] = all_confidences\n",
    "association_rules_df['collective strength'] = collective_strengths\n",
    "association_rules_df['cosine similarity'] = cosine_similarities\n",
    "\n",
    "df_association = association_rules_df.sort_values(by='lift',ascending=False)\n",
    "\n",
    "\n",
    "df_association\n",
    "\n",
    "df_association.to_csv('../data/연관규칙 30대.csv', encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "def28d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 363 combinations | Sampling itemset size 32\n"
     ]
    }
   ],
   "source": [
    "df= pd.read_csv(\"../data/구매상품데이터프레임 40대.csv\",index_col=0)\n",
    "\n",
    "records = []\n",
    "for i in range(len(df)):\n",
    "    records.append([str(df.values[i,j]) \\\n",
    "                    for j in range(len(df.columns)) if not pd.isna(df.values[i,j])])\n",
    "\n",
    "\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(records).transform(records, sparse=True)\n",
    "te_df = pd.DataFrame.sparse.from_spmatrix(te_ary, columns=te.columns_)\n",
    "\n",
    "\n",
    "frequent_itemset = apriori(te_df,\n",
    "                           min_support=0.005, \n",
    "                           max_len=3, \n",
    "                           use_colnames=True, \n",
    "                           verbose=1 \n",
    "                          )\n",
    "frequent_itemset['length'] = frequent_itemset['itemsets'].map(lambda x: len(x))\n",
    "frequent_itemset.sort_values('support',ascending=False,inplace=True)\n",
    "\n",
    "\n",
    "association_rules_df = association_rules(frequent_itemset, \n",
    "                                         metric='confidence', \n",
    "                                         min_threshold=0.005,\n",
    "                                        )\n",
    "all_confidences = []\n",
    "collective_strengths = []\n",
    "cosine_similarities = []\n",
    "for _,row in association_rules_df.iterrows():\n",
    "    all_confidence_if = list(row['antecedents'])[0]\n",
    "    all_confidence_then = list(row['consequents'])[0]\n",
    "    if row['antecedent support'] <= row['consequent support']:\n",
    "        all_confidence_if = list(row['consequents'])[0]\n",
    "        all_confidence_then = list(row['antecedents'])[0]\n",
    "    all_confidence = {all_confidence_if+' => '+all_confidence_then : \\\n",
    "                      row['support']/max(row['antecedent support'], row['consequent support'])}\n",
    "    all_confidences.append(all_confidence)\n",
    "    \n",
    "    violation = row['antecedent support'] + row['consequent support'] - 2*row['support']\n",
    "    ex_violation = 1-row['antecedent support']*row['consequent support'] - \\\n",
    "                    (1-row['antecedent support'])*(1-row['consequent support'])\n",
    "    if violation == 0 or ex_violation == 0:\n",
    "        collective_strength = 0\n",
    "    else:\n",
    "        collective_strength = (1-violation)/(1-ex_violation)*(ex_violation/violation+1)+1\n",
    "    collective_strengths.append(collective_strength)\n",
    "    \n",
    "    cosine_similarity = row['support']/np.sqrt(row['antecedent support']*row['consequent support'])\n",
    "    cosine_similarities.append(cosine_similarity)\n",
    "    \n",
    "association_rules_df['all-confidence'] = all_confidences\n",
    "association_rules_df['collective strength'] = collective_strengths\n",
    "association_rules_df['cosine similarity'] = cosine_similarities\n",
    "\n",
    "df_association = association_rules_df.sort_values(by='lift',ascending=False)\n",
    "df_association\n",
    "\n",
    "df_association.to_csv('../data/연관규칙 40대.csv', encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c275e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 597 combinations | Sampling itemset size 32\n"
     ]
    }
   ],
   "source": [
    "df= pd.read_csv(\"../data/구매상품데이터프레임 50대.csv\",index_col=0)\n",
    "\n",
    "records = []\n",
    "for i in range(len(df)):\n",
    "    records.append([str(df.values[i,j]) \\\n",
    "                    for j in range(len(df.columns)) if not pd.isna(df.values[i,j])])\n",
    "\n",
    "\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(records).transform(records, sparse=True)\n",
    "te_df = pd.DataFrame.sparse.from_spmatrix(te_ary, columns=te.columns_)\n",
    "\n",
    "\n",
    "frequent_itemset = apriori(te_df,\n",
    "                           min_support=0.005, \n",
    "                           max_len=3, \n",
    "                           use_colnames=True, \n",
    "                           verbose=1 \n",
    "                          )\n",
    "frequent_itemset['length'] = frequent_itemset['itemsets'].map(lambda x: len(x))\n",
    "frequent_itemset.sort_values('support',ascending=False,inplace=True)\n",
    "\n",
    "\n",
    "association_rules_df = association_rules(frequent_itemset, \n",
    "                                         metric='confidence', \n",
    "                                         min_threshold=0.005,\n",
    "                                        )\n",
    "all_confidences = []\n",
    "collective_strengths = []\n",
    "cosine_similarities = []\n",
    "for _,row in association_rules_df.iterrows():\n",
    "    all_confidence_if = list(row['antecedents'])[0]\n",
    "    all_confidence_then = list(row['consequents'])[0]\n",
    "    if row['antecedent support'] <= row['consequent support']:\n",
    "        all_confidence_if = list(row['consequents'])[0]\n",
    "        all_confidence_then = list(row['antecedents'])[0]\n",
    "    all_confidence = {all_confidence_if+' => '+all_confidence_then : \\\n",
    "                      row['support']/max(row['antecedent support'], row['consequent support'])}\n",
    "    all_confidences.append(all_confidence)\n",
    "    \n",
    "    violation = row['antecedent support'] + row['consequent support'] - 2*row['support']\n",
    "    ex_violation = 1-row['antecedent support']*row['consequent support'] - \\\n",
    "                    (1-row['antecedent support'])*(1-row['consequent support'])\n",
    "    if violation == 0 or ex_violation == 0:\n",
    "        collective_strength = 0\n",
    "    else:\n",
    "        collective_strength = (1-violation)/(1-ex_violation)*(ex_violation/violation+1)+1\n",
    "    collective_strengths.append(collective_strength)\n",
    "    \n",
    "    cosine_similarity = row['support']/np.sqrt(row['antecedent support']*row['consequent support'])\n",
    "    cosine_similarities.append(cosine_similarity)\n",
    "    \n",
    "association_rules_df['all-confidence'] = all_confidences\n",
    "association_rules_df['collective strength'] = collective_strengths\n",
    "association_rules_df['cosine similarity'] = cosine_similarities\n",
    "\n",
    "df_association = association_rules_df.sort_values(by='lift',ascending=False)\n",
    "df_association\n",
    "\n",
    "df_association.to_csv('../data/연관규칙 50대.csv', encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "834601d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 459 combinations | Sampling itemset size 32\n"
     ]
    }
   ],
   "source": [
    "df= pd.read_csv(\"../data/구매상품데이터프레임 60대 이상.csv\",index_col=0)\n",
    "\n",
    "records = []\n",
    "for i in range(len(df)):\n",
    "    records.append([str(df.values[i,j]) \\\n",
    "                    for j in range(len(df.columns)) if not pd.isna(df.values[i,j])])\n",
    "\n",
    "\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(records).transform(records, sparse=True)\n",
    "te_df = pd.DataFrame.sparse.from_spmatrix(te_ary, columns=te.columns_)\n",
    "\n",
    "\n",
    "frequent_itemset = apriori(te_df,\n",
    "                           min_support=0.005, \n",
    "                           max_len=3, \n",
    "                           use_colnames=True, \n",
    "                           verbose=1 \n",
    "                          )\n",
    "frequent_itemset['length'] = frequent_itemset['itemsets'].map(lambda x: len(x))\n",
    "frequent_itemset.sort_values('support',ascending=False,inplace=True)\n",
    "\n",
    "\n",
    "association_rules_df = association_rules(frequent_itemset, \n",
    "                                         metric='confidence', \n",
    "                                         min_threshold=0.005,\n",
    "                                        )\n",
    "all_confidences = []\n",
    "collective_strengths = []\n",
    "cosine_similarities = []\n",
    "for _,row in association_rules_df.iterrows():\n",
    "    all_confidence_if = list(row['antecedents'])[0]\n",
    "    all_confidence_then = list(row['consequents'])[0]\n",
    "    if row['antecedent support'] <= row['consequent support']:\n",
    "        all_confidence_if = list(row['consequents'])[0]\n",
    "        all_confidence_then = list(row['antecedents'])[0]\n",
    "    all_confidence = {all_confidence_if+' => '+all_confidence_then : \\\n",
    "                      row['support']/max(row['antecedent support'], row['consequent support'])}\n",
    "    all_confidences.append(all_confidence)\n",
    "    \n",
    "    violation = row['antecedent support'] + row['consequent support'] - 2*row['support']\n",
    "    ex_violation = 1-row['antecedent support']*row['consequent support'] - \\\n",
    "                    (1-row['antecedent support'])*(1-row['consequent support'])\n",
    "    if violation == 0 or ex_violation == 0:\n",
    "        collective_strength = 0\n",
    "    else:\n",
    "        collective_strength = (1-violation)/(1-ex_violation)*(ex_violation/violation+1)+1\n",
    "    collective_strengths.append(collective_strength)\n",
    "    \n",
    "    cosine_similarity = row['support']/np.sqrt(row['antecedent support']*row['consequent support'])\n",
    "    cosine_similarities.append(cosine_similarity)\n",
    "    \n",
    "association_rules_df['all-confidence'] = all_confidences\n",
    "association_rules_df['collective strength'] = collective_strengths\n",
    "association_rules_df['cosine similarity'] = cosine_similarities\n",
    "\n",
    "df_association = association_rules_df.sort_values(by='lift',ascending=False)\n",
    "df_association\n",
    "\n",
    "df_association.to_csv('../data/연관규칙 60대 이상.csv', encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdba6b64",
   "metadata": {},
   "source": [
    "# 연령별 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea6823f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_20= pd.read_csv(\"../data/연관규칙 20대.csv\",index_col=0)\n",
    "df_30= pd.read_csv(\"../data/연관규칙 30대.csv\",index_col=0)\n",
    "df_40= pd.read_csv(\"../data/연관규칙 40대.csv\",index_col=0)\n",
    "df_50= pd.read_csv(\"../data/연관규칙 50대.csv\",index_col=0)\n",
    "df_60= pd.read_csv(\"../data/연관규칙 60대 이상.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1f7c99b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_20['antecedents'] = df_20['antecedents'].str.replace('frozenset','')\n",
    "df_20['consequents'] = df_20['consequents'].str.replace('frozenset','')\n",
    "df_20 = df_20[['antecedents','consequents','lift']]\n",
    "df_30['antecedents'] = df_30['antecedents'].str.replace('frozenset','')\n",
    "df_30['consequents'] = df_30['consequents'].str.replace('frozenset','')\n",
    "df_30 = df_30[['antecedents','consequents','lift']]\n",
    "df_40['antecedents'] = df_40['antecedents'].str.replace('frozenset','')\n",
    "df_40['consequents'] = df_40['consequents'].str.replace('frozenset','')\n",
    "df_40 = df_40[['antecedents','consequents','lift']]\n",
    "df_50['antecedents'] = df_50['antecedents'].str.replace('frozenset','')\n",
    "df_50['consequents'] = df_50['consequents'].str.replace('frozenset','')\n",
    "df_50 = df_50[['antecedents','consequents','lift']]\n",
    "df_60['antecedents'] = df_60['antecedents'].str.replace('frozenset','')\n",
    "df_60['consequents'] = df_60['consequents'].str.replace('frozenset','')\n",
    "df_60 = df_60[['antecedents','consequents','lift']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a340f19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antecedents</th>\n",
       "      <th>consequents</th>\n",
       "      <th>lift</th>\n",
       "      <th>antecedents</th>\n",
       "      <th>consequents</th>\n",
       "      <th>lift</th>\n",
       "      <th>antecedents</th>\n",
       "      <th>consequents</th>\n",
       "      <th>lift</th>\n",
       "      <th>antecedents</th>\n",
       "      <th>consequents</th>\n",
       "      <th>lift</th>\n",
       "      <th>antecedents</th>\n",
       "      <th>consequents</th>\n",
       "      <th>lift</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>({'양파'})</td>\n",
       "      <td>({'당근'})</td>\n",
       "      <td>1.508483</td>\n",
       "      <td>({'오이'})</td>\n",
       "      <td>({'유정란/친환경'})</td>\n",
       "      <td>0.748312</td>\n",
       "      <td>({'유정란/친환경'})</td>\n",
       "      <td>({'당근'})</td>\n",
       "      <td>0.864176</td>\n",
       "      <td>({'유정란/친환경'})</td>\n",
       "      <td>({'당근'})</td>\n",
       "      <td>0.824755</td>\n",
       "      <td>({'중파'})</td>\n",
       "      <td>({'유정란/친환경'})</td>\n",
       "      <td>0.923496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>({'당근'})</td>\n",
       "      <td>({'양파'})</td>\n",
       "      <td>1.508483</td>\n",
       "      <td>({'유정란/친환경'})</td>\n",
       "      <td>({'오이'})</td>\n",
       "      <td>0.748312</td>\n",
       "      <td>({'당근'})</td>\n",
       "      <td>({'유정란/친환경'})</td>\n",
       "      <td>0.864176</td>\n",
       "      <td>({'당근'})</td>\n",
       "      <td>({'유정란/친환경'})</td>\n",
       "      <td>0.824755</td>\n",
       "      <td>({'유정란/친환경'})</td>\n",
       "      <td>({'중파'})</td>\n",
       "      <td>0.923496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>({'유정란/친환경'})</td>\n",
       "      <td>({'구운도시락김'})</td>\n",
       "      <td>1.167927</td>\n",
       "      <td>({'백미/유'})</td>\n",
       "      <td>({'유정란/친환경'})</td>\n",
       "      <td>0.750800</td>\n",
       "      <td>({'시금치'})</td>\n",
       "      <td>({'두부'})</td>\n",
       "      <td>0.920806</td>\n",
       "      <td>({'두부'})</td>\n",
       "      <td>({'대파'})</td>\n",
       "      <td>0.815532</td>\n",
       "      <td>({'두부'})</td>\n",
       "      <td>({'양배추'})</td>\n",
       "      <td>0.698674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>({'구운도시락김'})</td>\n",
       "      <td>({'유정란/친환경'})</td>\n",
       "      <td>1.167927</td>\n",
       "      <td>({'유정란/친환경'})</td>\n",
       "      <td>({'백미/유'})</td>\n",
       "      <td>0.750800</td>\n",
       "      <td>({'두부'})</td>\n",
       "      <td>({'시금치'})</td>\n",
       "      <td>0.920806</td>\n",
       "      <td>({'대파'})</td>\n",
       "      <td>({'두부'})</td>\n",
       "      <td>0.815532</td>\n",
       "      <td>({'양배추'})</td>\n",
       "      <td>({'두부'})</td>\n",
       "      <td>0.698674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>({'당근'})</td>\n",
       "      <td>({'콩나물'})</td>\n",
       "      <td>1.138070</td>\n",
       "      <td>({'유정란/친환경'})</td>\n",
       "      <td>({'유기농우유'})</td>\n",
       "      <td>0.823561</td>\n",
       "      <td>({'두부'})</td>\n",
       "      <td>({'찌개용두부'})</td>\n",
       "      <td>0.508570</td>\n",
       "      <td>({'두부'})</td>\n",
       "      <td>({'찌개용두부'})</td>\n",
       "      <td>0.532862</td>\n",
       "      <td>({'유정란/친환경'})</td>\n",
       "      <td>({'양배추'})</td>\n",
       "      <td>0.847283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>({'백미/유'})</td>\n",
       "      <td>({'유정란/친환경'})</td>\n",
       "      <td>0.732548</td>\n",
       "      <td>({'양파'})</td>\n",
       "      <td>({'오이'})</td>\n",
       "      <td>1.058318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>({'두부'})</td>\n",
       "      <td>({'애호박'})</td>\n",
       "      <td>0.725454</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>({'애호박'})</td>\n",
       "      <td>({'두부'})</td>\n",
       "      <td>0.725454</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>({'백미/유'})</td>\n",
       "      <td>({'두부'})</td>\n",
       "      <td>0.673721</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>({'두부'})</td>\n",
       "      <td>({'백미/유'})</td>\n",
       "      <td>0.673721</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      antecedents    consequents      lift    antecedents    consequents  \\\n",
       "19       ({'양파'})       ({'당근'})  1.508483       ({'오이'})  ({'유정란/친환경'})   \n",
       "18       ({'당근'})       ({'양파'})  1.508483  ({'유정란/친환경'})       ({'오이'})   \n",
       "39  ({'유정란/친환경'})   ({'구운도시락김'})  1.167927     ({'백미/유'})  ({'유정란/친환경'})   \n",
       "38   ({'구운도시락김'})  ({'유정란/친환경'})  1.167927  ({'유정란/친환경'})     ({'백미/유'})   \n",
       "27       ({'당근'})      ({'콩나물'})  1.138070  ({'유정란/친환경'})    ({'유기농우유'})   \n",
       "..            ...            ...       ...            ...            ...   \n",
       "55            NaN            NaN       NaN            NaN            NaN   \n",
       "96            NaN            NaN       NaN            NaN            NaN   \n",
       "97            NaN            NaN       NaN            NaN            NaN   \n",
       "75            NaN            NaN       NaN            NaN            NaN   \n",
       "74            NaN            NaN       NaN            NaN            NaN   \n",
       "\n",
       "        lift    antecedents    consequents      lift    antecedents  \\\n",
       "19  0.748312  ({'유정란/친환경'})       ({'당근'})  0.864176  ({'유정란/친환경'})   \n",
       "18  0.748312       ({'당근'})  ({'유정란/친환경'})  0.864176       ({'당근'})   \n",
       "39  0.750800      ({'시금치'})       ({'두부'})  0.920806       ({'두부'})   \n",
       "38  0.750800       ({'두부'})      ({'시금치'})  0.920806       ({'대파'})   \n",
       "27  0.823561       ({'두부'})    ({'찌개용두부'})  0.508570       ({'두부'})   \n",
       "..       ...            ...            ...       ...            ...   \n",
       "55       NaN            NaN            NaN       NaN     ({'백미/유'})   \n",
       "96       NaN            NaN            NaN       NaN       ({'두부'})   \n",
       "97       NaN            NaN            NaN       NaN      ({'애호박'})   \n",
       "75       NaN            NaN            NaN       NaN     ({'백미/유'})   \n",
       "74       NaN            NaN            NaN       NaN       ({'두부'})   \n",
       "\n",
       "      consequents      lift    antecedents    consequents      lift  \n",
       "19       ({'당근'})  0.824755       ({'중파'})  ({'유정란/친환경'})  0.923496  \n",
       "18  ({'유정란/친환경'})  0.824755  ({'유정란/친환경'})       ({'중파'})  0.923496  \n",
       "39       ({'대파'})  0.815532       ({'두부'})      ({'양배추'})  0.698674  \n",
       "38       ({'두부'})  0.815532      ({'양배추'})       ({'두부'})  0.698674  \n",
       "27    ({'찌개용두부'})  0.532862  ({'유정란/친환경'})      ({'양배추'})  0.847283  \n",
       "..            ...       ...            ...            ...       ...  \n",
       "55  ({'유정란/친환경'})  0.732548       ({'양파'})       ({'오이'})  1.058318  \n",
       "96      ({'애호박'})  0.725454            NaN            NaN       NaN  \n",
       "97       ({'두부'})  0.725454            NaN            NaN       NaN  \n",
       "75       ({'두부'})  0.673721            NaN            NaN       NaN  \n",
       "74     ({'백미/유'})  0.673721            NaN            NaN       NaN  \n",
       "\n",
       "[98 rows x 15 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.concat([df_20, df_30, df_40, df_50, df_60], axis=1)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415c0c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('../data/연관규칙 연령별 비교.csv', encoding=\"utf-8-sig\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
